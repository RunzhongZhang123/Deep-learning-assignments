{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? \n",
    "\n",
    "   Your answer: \n",
    "   **$$H(p, q)=- \\sum_x p(x) \\log q(x)$$\n",
    "   From the formula we can see that cross entropy can be used to calculate the difference between training data distribution and learning model distribution, so in classification, when the value of cross entropy is low, it means we have a model with distribution which can well presents the training data. Therefore, the loss can be written as following way:\n",
    "   $$L_i= - \\sum_k p_{i,k} \\log (\\frac{e^{f_k}}{\\sum_j e^{f_j}})$$\n",
    "   In classification, we calculate loss separately for each class label per observation and add up the result.**\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model.\n",
    "\n",
    "   Your answer: \n",
    "   \n",
    "   **In the binary logistic regression, we use $P(y=1)=\\sigma(W x + b)$ and predict 1 when the result is greater than 0.5, otherwise predicts 0 (here I only take '0-1' prediction as an example). However, after generalizing to multi-class logistic regression, we get the probabilities for every class by calculating the sigmoid of every class score, and choose the prediction with highest score. Here we assume the probabilities of each class are independent.**\n",
    "   \n",
    "   **For multi-class logistic regression there are mainly two appoarches: \"one vs all\" or \"one vs one\". As for \"one vs one\", we train $\\frac{(n-1)*n}{2}$ models, and each one makes prediction between two classes. Therefore, we have to run all these models on a single training sample and predict the class with the highest vote.**\n",
    "   \n",
    "   **In a deep learning classification model, the input data $x$ are fed into deep neural network and get features $x_{new}$, so logistic regression can be applied in the final layer for classification, taking $x_{new}$ as input and predicts labels.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: \n",
    "   \n",
    "   **Because of its linear and non-saturating form, ReLU is much easier to converge in SGD compared with sigmoid and other activation functions. Also, it is more plausible in the biological view. Finally, it is much faster because of the simple computation.**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "\n",
    "   Your answer: \n",
    "   \n",
    "   **First, randomly mix the training dataset and divide it into k groups. Second, for each individual group take it as the validation set and use it to tune the hyperparameters (so the rest are considered as training set). Third, repeat the second step above for all k groups. Finally, calculate the test accuracy on test set.**\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer:\n",
    "   \n",
    "   **My best model's parameters start from random variables, so the loss is high at the very beginning. By observing the former network in the assignment, I find out that the accuracy is still going up even at the end of trainning, so I increase number of epoches to 90. Also, to better fit the data, I increase the hidden dimension to 500. The result is very satisfying: the validation accuracy reaches 40% at epoch 6, and 45% at epoch 12. At epoch 71, the validation accuracy achieves 50%. Finally, the test accuracy turns out to be 50.87%.**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: \n",
    "   \n",
    "   **The perplexity shows how to balance attention between local and global aspects, and the motivation is to tune the parameter in order to make data with same label close to each other. In this assignment, after trying perplexity from 5 to 100, I find that value around 40 shows better result than others. Therefore, I choose 40 as the value of parameter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
